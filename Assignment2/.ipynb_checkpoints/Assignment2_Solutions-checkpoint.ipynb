{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "In Assignment 1, we studied how information is represented by a single spiking neuron. In this assignment, you will learn how to construct networks of spiking neurons for a given cognitive task, how to propagate information through a network, and understanding the intuition behind network design choices.Â \n",
    "\n",
    "Let's first import all the libraries required for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: From single neuron to network of neurons\n",
    "## 1a.\n",
    "What computational advantages do networks of neurons offer when compared against information processing by a single neuron? In other words, why do we need networks of neurons? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1a.\n",
    "Double click to enter your response to Question 1a here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. \n",
    "Describe algorithmically the information flow through a network of spiking leaky-integrate-and-fire (LIF) neurons. Specifically, trace out the steps required to compute network output from a given (continuous-valued) inputs. The algorithm should describe how continuous-valued inputs are fed to the SNN input layer, how the layer activations are computed, and how the output layer activity is decoded. Also, provide a diagrammatic overview of the algorithm to aid your explanation. You are free to assume any network size, and input and output dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1b.\n",
    "Double click to enter your response to Question 1b here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Elements of Constructing Feedforward Networks\n",
    "The fundamental components of a feedforward network are layers of neurons and connections between those layers. In this exercise, you will implement these two fundamental components of a feedforward spiking neural network.\n",
    "## 2a. \n",
    "As the first step towards creating an SNN, we will create a class that defines a layer of LIF neurons. The layer object creates a collection of LIF neurons and applies inputs to it to produce the collective spiking output of the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNeurons:\n",
    "    \"\"\" Define Leaky Integrate-and-Fire Neuron Layer \"\"\"\n",
    "\n",
    "    def __init__(self, dimension, vdecay, vth):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimension (int): Number of LIF neurons in the layer\n",
    "            vdecay (float): voltage decay of LIF neurons\n",
    "            vth (float): voltage threshold of LIF neurons\n",
    "        \n",
    "        This function is complete. You do not need to do anything here.\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.vdecay = vdecay\n",
    "        self.vth = vth\n",
    "\n",
    "        # Initialize LIF neuron states\n",
    "        self.volt = np.zeros(self.dimension)\n",
    "        self.spike = np.zeros(self.dimension)\n",
    "    \n",
    "    def __call__(self, psp_input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            psp_input (ndarray): synaptic inputs \n",
    "        Return:\n",
    "            self.spike: output spikes from the layer\n",
    "        \n",
    "        Write the expressions for updating the voltage and generating the spike. \n",
    "        \"\"\"\n",
    "        self.volt = self.vdecay * self.volt * (1. - self.spike) + psp_input\n",
    "        self.spike = (self.volt > self.vth).astype(float)\n",
    "        return self.spike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of your class implementation, create a layer of neurons using the class definition above, and pass through it random inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  [0 1 0 0 0 1 0 0 1 0]\n",
      "Output:  [0. 1. 0. 0. 0. 1. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Create a layer of neurons using the class definition above\n",
    "layer = LIFNeurons(10, 0.3, 0.75)\n",
    "\n",
    "#Create random input spikes and print them\n",
    "in_spikes = np.random.choice([0,1], 10, p=[0.6, 0.4])\n",
    "print('Inputs: ', in_spikes)\n",
    "\n",
    "#Propagate the random input spikes through the layer and print the output\n",
    "out = layer(in_spikes)\n",
    "print('Output: ', out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b.\n",
    "Now we will create a class the defines connections between the spiking neuron layers. The connection object takes in inputs as the activations of the presynaptic layer and the connection weights between the pre- and post-synaptic layers. The output is the activation of the postsynaptic layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connections:\n",
    "    \"\"\" Define connections between spiking neuron layers \"\"\"\n",
    "\n",
    "    def __init__(self, weights, pre_dimension, post_dimension):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            weights (ndarray): connection weights\n",
    "            pre_dimension (int): dimension for pre-synaptic neurons\n",
    "            post_dimension (int): dimension for post-synaptic neurons\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.pre_dimension = pre_dimension\n",
    "        self.post_dimension = post_dimension\n",
    "    \n",
    "    def __call__(self, spike_input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spike_input (ndarray): spikes generated by the pre-synaptic neurons\n",
    "        Return:\n",
    "            psp: postsynaptic layer activations\n",
    "        \"\"\"\n",
    "        psp = np.matmul(self.weights, spike_input)\n",
    "        return psp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of your class implementation, create a connection object and compute the activation of the postsynaptic layer for random presynaptic activation inputs and random connection weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "#Define the dimensions of the presynaptic layer in a variable\n",
    "pre_dim = 10\n",
    "\n",
    "#Define the dimensions of the postsynaptic layer in a variable\n",
    "post_dim = 100\n",
    "\n",
    "#Create random presynaptic inputs\n",
    "in_spikes = np.random.choice([0,1], 10, p=[0.6, 0.4])\n",
    "\n",
    "#Create a random connection weight matrix\n",
    "conn_weights = np.random.rand(100, 10)\n",
    "\n",
    "#Initialize a connection object using the Connection class definition and pass the variables created above as arguments\n",
    "conn = Connections(conn_weights, pre_dim, post_dim)\n",
    "\n",
    "#Compute the postsynaptic activation when the connection object is fed random presynaptic activation inputs\n",
    "psp = conn(in_spikes)\n",
    "\n",
    "#Print the shape of the postsynaptic activation\n",
    "print(psp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Constructing Feedforward SNN\n",
    "Now that you have implemented the basic elements of an SNN- layer and connection, you are all set to implement a fully functioning 1-layer SNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN:\n",
    "    \"\"\" Define a Spiking Neural Network with One Hidden Layer \"\"\"\n",
    "\n",
    "    def __init__(self, input_2_hidden_weight, hidden_2_output_weight, \n",
    "                 input_dimension=784, hidden_dimension=256, output_dimension=10,\n",
    "                 vdecay=0.5, vth=0.5, snn_timestep=20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_2_hidden_weight (ndarray): weights for connection between input and hidden layer\n",
    "            hidden_2_output_weight (ndarray): weights for connection between hidden and output layer\n",
    "            input_dimension (int): input dimension\n",
    "            hidden_dimension (int): hidden_dimension\n",
    "            output_dimension (int): output_dimension\n",
    "            vdecay (float): voltage decay of LIF neuron\n",
    "            vth (float): voltage threshold of LIF neuron\n",
    "            snn_timestep (int): number of timesteps for inference\n",
    "        \"\"\"\n",
    "        self.snn_timestep = snn_timestep\n",
    "        self.hidden_layer = LIFNeurons(hidden_dimension, vdecay, vth)\n",
    "        self.output_layer = LIFNeurons(output_dimension, vdecay, vth)\n",
    "        self.input_2_hidden_connection = Connections(input_2_hidden_weight, input_dimension, hidden_dimension)\n",
    "        self.hidden_2_output_connection = Connections(hidden_2_output_weight, hidden_dimension, output_dimension)\n",
    "    \n",
    "    def __call__(self, spike_encoding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spike_encoding (ndarray): spike encoding of input\n",
    "        Return:\n",
    "            spike outputs of the network\n",
    "        \"\"\"\n",
    "        spike_output = np.zeros(self.output_layer.dimension)\n",
    "        for tt in range(self.snn_timestep):\n",
    "            input_2_hidden_psp = self.input_2_hidden_connection(spike_encoding[:, tt])\n",
    "            hidden_spikes = self.hidden_layer(input_2_hidden_psp)\n",
    "            hidden_2_output_psp = self.hidden_2_output_connection(hidden_spikes)\n",
    "            output_spikes = self.output_layer(hidden_2_output_psp)\n",
    "            spike_output += output_spikes\n",
    "        return spike_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of your class implementation, define the arguments to initialize the SNN. Then initialize the SNN and pass through it random inputs and compute network outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  [[1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1]\n",
      " [0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 0 1]\n",
      " [0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1]\n",
      " [0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0]\n",
      " [0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0]\n",
      " [0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0]\n",
      " [1 0 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0]]\n",
      "Outputs:  [19. 19. 19. 19. 19. 19. 19. 19. 19. 19.]\n"
     ]
    }
   ],
   "source": [
    "input_dim = 10\n",
    "hid_dim = 100\n",
    "out_dim = 10\n",
    "vdecay=0.5\n",
    "vth=0.5\n",
    "snn_timestep=20\n",
    "in_2_hid_weight = np.random.rand(100, 10)\n",
    "hid_2_out_weight = np.random.rand(10, 100)\n",
    "spike_encoding = np.random.choice([0,1], (input_dim, snn_timestep), p=[0.6, 0.4])\n",
    "print('Inputs: ', spike_encoding)\n",
    "snn = SNN(in_2_hid_weight, hid_2_out_weight, input_dimension=input_dim, hidden_dimension=hid_dim, output_dimension=out_dim, vdecay=0.5, vth=0.5, snn_timestep=20)\n",
    "out = snn(spike_encoding)\n",
    "print('Outputs: ', out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: SNN for Classification of Digits\n",
    "So far we have learnt how to construct SNNs for random inputs. In this exercise, you will use your implementation of SNNs to classify real-world data, taking the dataset of handwritten digits as an example. The dataset is provided as numpy arrays in the folder data. \n",
    "\n",
    "## 4a. \n",
    "First, we need to write two helper functions- to read the data from the saved data files, and to convert an image into spikes. The function to read the data is already written for you. You need to complete the function for encoding the data into spikes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_numpy_mnist_data(save_root, num_sample):\n",
    "    \"\"\"\n",
    "    Read saved numpy MNIST data\n",
    "    Args:\n",
    "        save_root (str): path for save data\n",
    "        num_sample (int): number of samples to read\n",
    "    Returns:\n",
    "        image_list: list of MNIST image\n",
    "        label_list: list of labels\n",
    "    \"\"\"\n",
    "    image_list = np.zeros((num_sample, 28, 28))\n",
    "    label_list = []\n",
    "    for ii in range(num_sample):\n",
    "        image_label = pickle.load(open(save_root + '/' + str(ii) + '.p', 'rb'))\n",
    "        image_list[ii] = image_label[0]\n",
    "        label_list.append(image_label[1])\n",
    "\n",
    "    return image_list, label_list\n",
    "\n",
    "def img_2_event_img(image, snn_timestep):\n",
    "    \"\"\"\n",
    "    Transform image to event image\n",
    "    Args:\n",
    "        image (ndarray): image\n",
    "        snn_timestep (int): spike timestep\n",
    "    Returns:\n",
    "        event_image: event image\n",
    "    \"\"\"\n",
    "    batch_size = image.shape[0]\n",
    "    image_size = image.shape[2]\n",
    "    image = image.reshape(batch_size, image_size, image_size, 1)\n",
    "    random_image = np.random.rand(batch_size, image_size, image_size, snn_timestep)\n",
    "    event_image = (random_image < image).astype(float)\n",
    "\n",
    "    return event_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of your class implementation, load a sample digit from the saved file and convert it into an event image. Then print the shape of the event image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 28, 28)\n",
      "(5, 28, 28, 20)\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_numpy_mnist_data(\"../numpy-mnist-snn/data/mnist_test\", 5)\n",
    "print(data.shape)\n",
    "event_img = img_2_event_img(data, 20)\n",
    "print(event_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. \n",
    "Next, we need another helper function to compute the classification accuracy of the network for given samples on MNIST digits. The function takes in as arguments the SNN, directory in which the data is saved, and the number of samples to take from the MNIST dataset. Your task is to use the helper functions created above to load the data, convert to event images, and then compute network prediction and accuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_snn_with_mnist(network, data_save_dir, data_sample_num):\n",
    "    \"\"\"\n",
    "    Test SNN with MNIST test data\n",
    "    Args:\n",
    "        network (SNN): defined SNN network\n",
    "        data_save_dir (str): directory for the test data\n",
    "        data_sample_num (int): number of test data\n",
    "    \"\"\"\n",
    "    test_image_list, test_label_list = read_numpy_mnist_data(data_save_dir, data_sample_num)\n",
    "    test_event_image_list = img_2_event_img(test_image_list, network.snn_timestep)\n",
    "    correct_predition = 0\n",
    "    for ii in range(data_sample_num):\n",
    "        snn_output = network(test_event_image_list[ii].reshape(-1, network.snn_timestep))\n",
    "        pred_label = np.argmax(snn_output)\n",
    "        if pred_label == test_label_list[ii]:\n",
    "            correct_predition += 1\n",
    "    test_accuracy = correct_predition / data_sample_num\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Tuning Membrane Properties for Correct Classification \n",
    "Great! We have everything that we need to measure the performance of the SNN for classification of MNIST digits. For this, we first need to create the SNN using the class definition we wrote in Q.3. Then we need to call the test function that we wrote in Q.4b. However, note that the SNN needs the connection weights between the layers as inputs. These weights are typically obtained as a result of \"training\" the network for a given task (such as MNIST classification). However, since training the network isn't a part of this assignment, we provide to you already trained weights. \n",
    "\n",
    "## 5a. \n",
    "Your task in this exercise is to initialize an SNN with vdecay=1.0 and vth=0.5. Test the SNN on MNIST dataset and obtain the classification accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.248\n"
     ]
    }
   ],
   "source": [
    "snn_param_dir = '../numpy-mnist-snn/save_models/snn_bptt_mnist_train.p'\n",
    "snn_param_dict = pickle.load(open(snn_param_dir, 'rb'))\n",
    "input_2_hidden_weight = snn_param_dict['weight1']\n",
    "hidden_2_output_weight = snn_param_dict['weight2']\n",
    "vdecay = 1.0\n",
    "vth = 0.5\n",
    "snn = SNN(input_2_hidden_weight, hidden_2_output_weight, vdecay=vdecay, vth=vth)\n",
    "test_acc = test_snn_with_mnist(snn, '../numpy-mnist-snn/data/mnist_test', 1000)\n",
    "print(\"Test Accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could be a possible reason for the poor accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 5a. \n",
    "Double click to enter your reponse ot Question 5a. here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. \n",
    "Can you tune the membrane properties (vdecay and vth) to obtain higher classification accuracies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your implementation of Question 5b. here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5c.\n",
    "Based on your response to Questions 5a. and 5b., can you explain how membrane properties affect network activity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 5c.\n",
    "Double click to enter your response to Question 5c. here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
